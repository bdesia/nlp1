{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "## Procesamiento del Lenguaje Natural 1\n",
    "### Carrera de Especialización en Inteligencia Artificial - FIUBA\n",
    "\n",
    "## Desafío N° 2\n",
    "### Custom embeddings con Gensim\n",
    "\n",
    "### 2º Bimestre 2025\n",
    "\n",
    "### Grupo\n",
    "\n",
    "| Autores               | E-mail                    | Nº SIU  |\n",
    "|---------------------- |---------------------------|---------|\n",
    "| Braian Desía          | b.desia@hotmail.com       | a1804   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7nqkumo9z9"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07zJxG7H9vG"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos un dataset de texto de reviews de peliculas extraído de Kaggle. MDB es un dataset que contienee 50k reviews de peliculas para NLP con una clasificación binaria del sentimiento de la crítica.\n",
    "\n",
    "Más información del dataset puede encontrarse en http://ai.stanford.edu/~amaas/data/sentiment/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\badesia\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "mysGrIw9ljC2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMDB Dataset.csv']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Dataset name\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ticoqYD1Z3I7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armar el dataset. Utilizamos \",\" para separar el review del sentimiento.\n",
    "df = pd.read_csv(os.path.join(path, 'IMDB Dataset.csv'), sep=',')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "LEpKubK9XzXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab94qaFlrA1G"
   },
   "source": [
    "### Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "rIsmMWmjrDHd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badesia\\AppData\\Local\\Temp\\ipykernel_19536\\1072949605.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  tokens = text_to_word_sequence(row[0])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence_tokens = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tokens = text_to_word_sequence(row[0])\n",
    "    sentence_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CHepi_DGrbhq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'other',\n",
       "  'reviewers',\n",
       "  'has',\n",
       "  'mentioned',\n",
       "  'that',\n",
       "  'after',\n",
       "  'watching',\n",
       "  'just',\n",
       "  '1',\n",
       "  'oz',\n",
       "  'episode',\n",
       "  \"you'll\",\n",
       "  'be',\n",
       "  'hooked',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'as',\n",
       "  'this',\n",
       "  'is',\n",
       "  'exactly',\n",
       "  'what',\n",
       "  'happened',\n",
       "  'with',\n",
       "  'me',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'first',\n",
       "  'thing',\n",
       "  'that',\n",
       "  'struck',\n",
       "  'me',\n",
       "  'about',\n",
       "  'oz',\n",
       "  'was',\n",
       "  'its',\n",
       "  'brutality',\n",
       "  'and',\n",
       "  'unflinching',\n",
       "  'scenes',\n",
       "  'of',\n",
       "  'violence',\n",
       "  'which',\n",
       "  'set',\n",
       "  'in',\n",
       "  'right',\n",
       "  'from',\n",
       "  'the',\n",
       "  'word',\n",
       "  'go',\n",
       "  'trust',\n",
       "  'me',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'show',\n",
       "  'for',\n",
       "  'the',\n",
       "  'faint',\n",
       "  'hearted',\n",
       "  'or',\n",
       "  'timid',\n",
       "  'this',\n",
       "  'show',\n",
       "  'pulls',\n",
       "  'no',\n",
       "  'punches',\n",
       "  'with',\n",
       "  'regards',\n",
       "  'to',\n",
       "  'drugs',\n",
       "  'sex',\n",
       "  'or',\n",
       "  'violence',\n",
       "  'its',\n",
       "  'is',\n",
       "  'hardcore',\n",
       "  'in',\n",
       "  'the',\n",
       "  'classic',\n",
       "  'use',\n",
       "  'of',\n",
       "  'the',\n",
       "  'word',\n",
       "  'br',\n",
       "  'br',\n",
       "  'it',\n",
       "  'is',\n",
       "  'called',\n",
       "  'oz',\n",
       "  'as',\n",
       "  'that',\n",
       "  'is',\n",
       "  'the',\n",
       "  'nickname',\n",
       "  'given',\n",
       "  'to',\n",
       "  'the',\n",
       "  'oswald',\n",
       "  'maximum',\n",
       "  'security',\n",
       "  'state',\n",
       "  'penitentary',\n",
       "  'it',\n",
       "  'focuses',\n",
       "  'mainly',\n",
       "  'on',\n",
       "  'emerald',\n",
       "  'city',\n",
       "  'an',\n",
       "  'experimental',\n",
       "  'section',\n",
       "  'of',\n",
       "  'the',\n",
       "  'prison',\n",
       "  'where',\n",
       "  'all',\n",
       "  'the',\n",
       "  'cells',\n",
       "  'have',\n",
       "  'glass',\n",
       "  'fronts',\n",
       "  'and',\n",
       "  'face',\n",
       "  'inwards',\n",
       "  'so',\n",
       "  'privacy',\n",
       "  'is',\n",
       "  'not',\n",
       "  'high',\n",
       "  'on',\n",
       "  'the',\n",
       "  'agenda',\n",
       "  'em',\n",
       "  'city',\n",
       "  'is',\n",
       "  'home',\n",
       "  'to',\n",
       "  'many',\n",
       "  'aryans',\n",
       "  'muslims',\n",
       "  'gangstas',\n",
       "  'latinos',\n",
       "  'christians',\n",
       "  'italians',\n",
       "  'irish',\n",
       "  'and',\n",
       "  'more',\n",
       "  'so',\n",
       "  'scuffles',\n",
       "  'death',\n",
       "  'stares',\n",
       "  'dodgy',\n",
       "  'dealings',\n",
       "  'and',\n",
       "  'shady',\n",
       "  'agreements',\n",
       "  'are',\n",
       "  'never',\n",
       "  'far',\n",
       "  'away',\n",
       "  'br',\n",
       "  'br',\n",
       "  'i',\n",
       "  'would',\n",
       "  'say',\n",
       "  'the',\n",
       "  'main',\n",
       "  'appeal',\n",
       "  'of',\n",
       "  'the',\n",
       "  'show',\n",
       "  'is',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'it',\n",
       "  'goes',\n",
       "  'where',\n",
       "  'other',\n",
       "  'shows',\n",
       "  \"wouldn't\",\n",
       "  'dare',\n",
       "  'forget',\n",
       "  'pretty',\n",
       "  'pictures',\n",
       "  'painted',\n",
       "  'for',\n",
       "  'mainstream',\n",
       "  'audiences',\n",
       "  'forget',\n",
       "  'charm',\n",
       "  'forget',\n",
       "  'romance',\n",
       "  'oz',\n",
       "  \"doesn't\",\n",
       "  'mess',\n",
       "  'around',\n",
       "  'the',\n",
       "  'first',\n",
       "  'episode',\n",
       "  'i',\n",
       "  'ever',\n",
       "  'saw',\n",
       "  'struck',\n",
       "  'me',\n",
       "  'as',\n",
       "  'so',\n",
       "  'nasty',\n",
       "  'it',\n",
       "  'was',\n",
       "  'surreal',\n",
       "  'i',\n",
       "  \"couldn't\",\n",
       "  'say',\n",
       "  'i',\n",
       "  'was',\n",
       "  'ready',\n",
       "  'for',\n",
       "  'it',\n",
       "  'but',\n",
       "  'as',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'more',\n",
       "  'i',\n",
       "  'developed',\n",
       "  'a',\n",
       "  'taste',\n",
       "  'for',\n",
       "  'oz',\n",
       "  'and',\n",
       "  'got',\n",
       "  'accustomed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'high',\n",
       "  'levels',\n",
       "  'of',\n",
       "  'graphic',\n",
       "  'violence',\n",
       "  'not',\n",
       "  'just',\n",
       "  'violence',\n",
       "  'but',\n",
       "  'injustice',\n",
       "  'crooked',\n",
       "  'guards',\n",
       "  \"who'll\",\n",
       "  'be',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'for',\n",
       "  'a',\n",
       "  'nickel',\n",
       "  'inmates',\n",
       "  \"who'll\",\n",
       "  'kill',\n",
       "  'on',\n",
       "  'order',\n",
       "  'and',\n",
       "  'get',\n",
       "  'away',\n",
       "  'with',\n",
       "  'it',\n",
       "  'well',\n",
       "  'mannered',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'inmates',\n",
       "  'being',\n",
       "  'turned',\n",
       "  'into',\n",
       "  'prison',\n",
       "  'bitches',\n",
       "  'due',\n",
       "  'to',\n",
       "  'their',\n",
       "  'lack',\n",
       "  'of',\n",
       "  'street',\n",
       "  'skills',\n",
       "  'or',\n",
       "  'prison',\n",
       "  'experience',\n",
       "  'watching',\n",
       "  'oz',\n",
       "  'you',\n",
       "  'may',\n",
       "  'become',\n",
       "  'comfortable',\n",
       "  'with',\n",
       "  'what',\n",
       "  'is',\n",
       "  'uncomfortable',\n",
       "  'viewing',\n",
       "  'thats',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'get',\n",
       "  'in',\n",
       "  'touch',\n",
       "  'with',\n",
       "  'your',\n",
       "  'darker',\n",
       "  'side'],\n",
       " ['a',\n",
       "  'wonderful',\n",
       "  'little',\n",
       "  'production',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'filming',\n",
       "  'technique',\n",
       "  'is',\n",
       "  'very',\n",
       "  'unassuming',\n",
       "  'very',\n",
       "  'old',\n",
       "  'time',\n",
       "  'bbc',\n",
       "  'fashion',\n",
       "  'and',\n",
       "  'gives',\n",
       "  'a',\n",
       "  'comforting',\n",
       "  'and',\n",
       "  'sometimes',\n",
       "  'discomforting',\n",
       "  'sense',\n",
       "  'of',\n",
       "  'realism',\n",
       "  'to',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'piece',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'actors',\n",
       "  'are',\n",
       "  'extremely',\n",
       "  'well',\n",
       "  'chosen',\n",
       "  'michael',\n",
       "  'sheen',\n",
       "  'not',\n",
       "  'only',\n",
       "  'has',\n",
       "  'got',\n",
       "  'all',\n",
       "  'the',\n",
       "  'polari',\n",
       "  'but',\n",
       "  'he',\n",
       "  'has',\n",
       "  'all',\n",
       "  'the',\n",
       "  'voices',\n",
       "  'down',\n",
       "  'pat',\n",
       "  'too',\n",
       "  'you',\n",
       "  'can',\n",
       "  'truly',\n",
       "  'see',\n",
       "  'the',\n",
       "  'seamless',\n",
       "  'editing',\n",
       "  'guided',\n",
       "  'by',\n",
       "  'the',\n",
       "  'references',\n",
       "  'to',\n",
       "  \"williams'\",\n",
       "  'diary',\n",
       "  'entries',\n",
       "  'not',\n",
       "  'only',\n",
       "  'is',\n",
       "  'it',\n",
       "  'well',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'watching',\n",
       "  'but',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'terrificly',\n",
       "  'written',\n",
       "  'and',\n",
       "  'performed',\n",
       "  'piece',\n",
       "  'a',\n",
       "  'masterful',\n",
       "  'production',\n",
       "  'about',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'great',\n",
       "  \"master's\",\n",
       "  'of',\n",
       "  'comedy',\n",
       "  'and',\n",
       "  'his',\n",
       "  'life',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'realism',\n",
       "  'really',\n",
       "  'comes',\n",
       "  'home',\n",
       "  'with',\n",
       "  'the',\n",
       "  'little',\n",
       "  'things',\n",
       "  'the',\n",
       "  'fantasy',\n",
       "  'of',\n",
       "  'the',\n",
       "  'guard',\n",
       "  'which',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'use',\n",
       "  'the',\n",
       "  'traditional',\n",
       "  \"'dream'\",\n",
       "  'techniques',\n",
       "  'remains',\n",
       "  'solid',\n",
       "  'then',\n",
       "  'disappears',\n",
       "  'it',\n",
       "  'plays',\n",
       "  'on',\n",
       "  'our',\n",
       "  'knowledge',\n",
       "  'and',\n",
       "  'our',\n",
       "  'senses',\n",
       "  'particularly',\n",
       "  'with',\n",
       "  'the',\n",
       "  'scenes',\n",
       "  'concerning',\n",
       "  'orton',\n",
       "  'and',\n",
       "  'halliwell',\n",
       "  'and',\n",
       "  'the',\n",
       "  'sets',\n",
       "  'particularly',\n",
       "  'of',\n",
       "  'their',\n",
       "  'flat',\n",
       "  'with',\n",
       "  \"halliwell's\",\n",
       "  'murals',\n",
       "  'decorating',\n",
       "  'every',\n",
       "  'surface',\n",
       "  'are',\n",
       "  'terribly',\n",
       "  'well',\n",
       "  'done']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo\n",
    "sentence_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "### Vectorización (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0wnDdv9sJ47"
   },
   "outputs": [],
   "source": [
    "# Creamos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model = Word2Vec(min_count=5,       # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,          # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=300,   # dimensionalidad de los vectores \n",
    "                     negative=20,       # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=-1,        # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)              # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5lTt8wErsf17"
   },
   "outputs": [],
   "source": [
    "# Obtener el vocabulario con los tokens\n",
    "w2v_model.build_vocab(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TNc9qt4os5AT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 50000\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "idw9cHF3tSMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus: 42635\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la relación cantidad de words / tamaño embedding está en el orden de 140."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC9mZ8DPk-UC"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QSp-x0PAsq56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.0\n",
      "Loss after epoch 1: 0.0\n",
      "Loss after epoch 2: 0.0\n",
      "Loss after epoch 3: 0.0\n",
      "Loss after epoch 4: 0.0\n",
      "Loss after epoch 5: 0.0\n",
      "Loss after epoch 6: 0.0\n",
      "Loss after epoch 7: 0.0\n",
      "Loss after epoch 8: 0.0\n",
      "Loss after epoch 9: 0.0\n",
      "Loss after epoch 10: 0.0\n",
      "Loss after epoch 11: 0.0\n",
      "Loss after epoch 12: 0.0\n",
      "Loss after epoch 13: 0.0\n",
      "Loss after epoch 14: 0.0\n",
      "Loss after epoch 15: 0.0\n",
      "Loss after epoch 16: 0.0\n",
      "Loss after epoch 17: 0.0\n",
      "Loss after epoch 18: 0.0\n",
      "Loss after epoch 19: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model.train(sentence_tokens,\n",
    "                 total_examples=w2v_model.corpus_count,\n",
    "                 epochs=20,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddT9NVuNlCAe"
   },
   "source": [
    "### 4 - Ensayar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "6cHN9xGLuPEm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('toasters', 0.2472677230834961),\n",
       " ('stride', 0.23483218252658844),\n",
       " ('sensual', 0.2308262139558792),\n",
       " ('cline', 0.22364233434200287),\n",
       " ('prevail', 0.2161916047334671),\n",
       " ('miraculously', 0.2158774584531784),\n",
       " (\"'put\", 0.21401330828666687),\n",
       " ('jingoistic', 0.21126540005207062),\n",
       " ('ornament', 0.21089081466197968),\n",
       " ('prototypical', 0.210261270403862)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"amazing\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "47HiU5gdkdMq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enrico', 0.21665599942207336),\n",
       " ('busters', 0.21490000188350677),\n",
       " ('infestation', 0.2143319696187973),\n",
       " (\"candidate'\", 0.21313492953777313),\n",
       " ('rawness', 0.20819006860256195),\n",
       " ('cinderella', 0.20753347873687744),\n",
       " ('evigan', 0.207081139087677),\n",
       " (\"'big'\", 0.20440497994422913),\n",
       " ('lovett', 0.20092010498046875),\n",
       " ('128', 0.20087271928787231)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MENOS se relacionan con...:\n",
    "w2v_model.wv.most_similar(negative=[\"disappointing\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g8UVWe6lFmh"
   },
   "source": [
    "### 5 - Visualización de agrupación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "pDxEVXAivjr9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCCXtDpcugmd"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m vecs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m MAX_WORDS\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      8\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mscatter(x\u001b[38;5;241m=\u001b[39mvecs[:MAX_WORDS,\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mvecs[:MAX_WORDS,\u001b[38;5;241m1\u001b[39m], text\u001b[38;5;241m=\u001b[39mlabels[:MAX_WORDS])\n",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m, in \u001b[0;36mreduce_dimensions\u001b[1;34m(model, num_dimensions)\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mindex_to_key)  \n\u001b[0;32m     10\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39mnum_dimensions, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vectors, labels\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1176\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1044\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1112\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m   1111\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter_without_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_without_progress\n\u001b[1;32m-> 1112\u001b[0m     params, kl_divergence, it \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# Save the final number of iterations\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m it\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:403\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[0;32m    401\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 403\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m inc \u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    406\u001b[0m dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minvert(inc)\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:284\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    281\u001b[0m indptr \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    283\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_embedded\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 284\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (degrees_of_freedom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m degrees_of_freedom\n\u001b[0;32m    298\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 3D\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model,3)\n",
    "\n",
    "fig = px.scatter_3d(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], z=vecs[:MAX_WORDS,2],text=labels[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También se pueden guardar los vectores y labels como tsv para graficar en\n",
    "# http://projector.tensorflow.org/\n",
    "\n",
    "\n",
    "vectors = np.asarray(w2v_model.wv.vectors)\n",
    "labels = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
    "\n",
    "with open(\"labels.tsv\", \"w\") as fp:\n",
    "    for item in labels:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
