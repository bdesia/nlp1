{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "## Procesamiento del Lenguaje Natural 1\n",
    "### Carrera de Especialización en Inteligencia Artificial - FIUBA\n",
    "\n",
    "## Desafío N° 2\n",
    "### Custom embeddings con Gensim\n",
    "\n",
    "### 2º Bimestre 2025\n",
    "\n",
    "### Grupo\n",
    "\n",
    "| Autores               | E-mail                    | Nº SIU  |\n",
    "|---------------------- |---------------------------|---------|\n",
    "| Braian Desía          | b.desia@hotmail.com       | a1804   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7nqkumo9z9"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizará canciones de bandas para generar los embeddings, es decir, que los vectores tendrán la forma en función de como esa banda haya utilizado las palabras en sus canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07zJxG7H9vG"
   },
   "source": [
    "### Datos\n",
    "Utilizamos un dataset de texto de reviews de peliculas extraído de Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
    "For more dataset information, please go through the following link,\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\badesia\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mysGrIw9ljC2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMDB Dataset.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posibles bandas\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ticoqYD1Z3I7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armar el dataset. Utilizamos \",\" para separar el review del sentimiento.\n",
    "df = pd.read_csv(os.path.join(path, 'IMDB Dataset.csv'), sep=',')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LEpKubK9XzXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab94qaFlrA1G"
   },
   "source": [
    "### 1 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rIsmMWmjrDHd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badesia\\AppData\\Local\\Temp\\ipykernel_7492\\1072949605.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  tokens = text_to_word_sequence(row[0])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence_tokens = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tokens = text_to_word_sequence(row[0])\n",
    "    sentence_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "CHepi_DGrbhq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'other',\n",
       "  'reviewers',\n",
       "  'has',\n",
       "  'mentioned',\n",
       "  'that',\n",
       "  'after',\n",
       "  'watching',\n",
       "  'just',\n",
       "  '1',\n",
       "  'oz',\n",
       "  'episode',\n",
       "  \"you'll\",\n",
       "  'be',\n",
       "  'hooked',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'as',\n",
       "  'this',\n",
       "  'is',\n",
       "  'exactly',\n",
       "  'what',\n",
       "  'happened',\n",
       "  'with',\n",
       "  'me',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'first',\n",
       "  'thing',\n",
       "  'that',\n",
       "  'struck',\n",
       "  'me',\n",
       "  'about',\n",
       "  'oz',\n",
       "  'was',\n",
       "  'its',\n",
       "  'brutality',\n",
       "  'and',\n",
       "  'unflinching',\n",
       "  'scenes',\n",
       "  'of',\n",
       "  'violence',\n",
       "  'which',\n",
       "  'set',\n",
       "  'in',\n",
       "  'right',\n",
       "  'from',\n",
       "  'the',\n",
       "  'word',\n",
       "  'go',\n",
       "  'trust',\n",
       "  'me',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'show',\n",
       "  'for',\n",
       "  'the',\n",
       "  'faint',\n",
       "  'hearted',\n",
       "  'or',\n",
       "  'timid',\n",
       "  'this',\n",
       "  'show',\n",
       "  'pulls',\n",
       "  'no',\n",
       "  'punches',\n",
       "  'with',\n",
       "  'regards',\n",
       "  'to',\n",
       "  'drugs',\n",
       "  'sex',\n",
       "  'or',\n",
       "  'violence',\n",
       "  'its',\n",
       "  'is',\n",
       "  'hardcore',\n",
       "  'in',\n",
       "  'the',\n",
       "  'classic',\n",
       "  'use',\n",
       "  'of',\n",
       "  'the',\n",
       "  'word',\n",
       "  'br',\n",
       "  'br',\n",
       "  'it',\n",
       "  'is',\n",
       "  'called',\n",
       "  'oz',\n",
       "  'as',\n",
       "  'that',\n",
       "  'is',\n",
       "  'the',\n",
       "  'nickname',\n",
       "  'given',\n",
       "  'to',\n",
       "  'the',\n",
       "  'oswald',\n",
       "  'maximum',\n",
       "  'security',\n",
       "  'state',\n",
       "  'penitentary',\n",
       "  'it',\n",
       "  'focuses',\n",
       "  'mainly',\n",
       "  'on',\n",
       "  'emerald',\n",
       "  'city',\n",
       "  'an',\n",
       "  'experimental',\n",
       "  'section',\n",
       "  'of',\n",
       "  'the',\n",
       "  'prison',\n",
       "  'where',\n",
       "  'all',\n",
       "  'the',\n",
       "  'cells',\n",
       "  'have',\n",
       "  'glass',\n",
       "  'fronts',\n",
       "  'and',\n",
       "  'face',\n",
       "  'inwards',\n",
       "  'so',\n",
       "  'privacy',\n",
       "  'is',\n",
       "  'not',\n",
       "  'high',\n",
       "  'on',\n",
       "  'the',\n",
       "  'agenda',\n",
       "  'em',\n",
       "  'city',\n",
       "  'is',\n",
       "  'home',\n",
       "  'to',\n",
       "  'many',\n",
       "  'aryans',\n",
       "  'muslims',\n",
       "  'gangstas',\n",
       "  'latinos',\n",
       "  'christians',\n",
       "  'italians',\n",
       "  'irish',\n",
       "  'and',\n",
       "  'more',\n",
       "  'so',\n",
       "  'scuffles',\n",
       "  'death',\n",
       "  'stares',\n",
       "  'dodgy',\n",
       "  'dealings',\n",
       "  'and',\n",
       "  'shady',\n",
       "  'agreements',\n",
       "  'are',\n",
       "  'never',\n",
       "  'far',\n",
       "  'away',\n",
       "  'br',\n",
       "  'br',\n",
       "  'i',\n",
       "  'would',\n",
       "  'say',\n",
       "  'the',\n",
       "  'main',\n",
       "  'appeal',\n",
       "  'of',\n",
       "  'the',\n",
       "  'show',\n",
       "  'is',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'it',\n",
       "  'goes',\n",
       "  'where',\n",
       "  'other',\n",
       "  'shows',\n",
       "  \"wouldn't\",\n",
       "  'dare',\n",
       "  'forget',\n",
       "  'pretty',\n",
       "  'pictures',\n",
       "  'painted',\n",
       "  'for',\n",
       "  'mainstream',\n",
       "  'audiences',\n",
       "  'forget',\n",
       "  'charm',\n",
       "  'forget',\n",
       "  'romance',\n",
       "  'oz',\n",
       "  \"doesn't\",\n",
       "  'mess',\n",
       "  'around',\n",
       "  'the',\n",
       "  'first',\n",
       "  'episode',\n",
       "  'i',\n",
       "  'ever',\n",
       "  'saw',\n",
       "  'struck',\n",
       "  'me',\n",
       "  'as',\n",
       "  'so',\n",
       "  'nasty',\n",
       "  'it',\n",
       "  'was',\n",
       "  'surreal',\n",
       "  'i',\n",
       "  \"couldn't\",\n",
       "  'say',\n",
       "  'i',\n",
       "  'was',\n",
       "  'ready',\n",
       "  'for',\n",
       "  'it',\n",
       "  'but',\n",
       "  'as',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'more',\n",
       "  'i',\n",
       "  'developed',\n",
       "  'a',\n",
       "  'taste',\n",
       "  'for',\n",
       "  'oz',\n",
       "  'and',\n",
       "  'got',\n",
       "  'accustomed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'high',\n",
       "  'levels',\n",
       "  'of',\n",
       "  'graphic',\n",
       "  'violence',\n",
       "  'not',\n",
       "  'just',\n",
       "  'violence',\n",
       "  'but',\n",
       "  'injustice',\n",
       "  'crooked',\n",
       "  'guards',\n",
       "  \"who'll\",\n",
       "  'be',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'for',\n",
       "  'a',\n",
       "  'nickel',\n",
       "  'inmates',\n",
       "  \"who'll\",\n",
       "  'kill',\n",
       "  'on',\n",
       "  'order',\n",
       "  'and',\n",
       "  'get',\n",
       "  'away',\n",
       "  'with',\n",
       "  'it',\n",
       "  'well',\n",
       "  'mannered',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'inmates',\n",
       "  'being',\n",
       "  'turned',\n",
       "  'into',\n",
       "  'prison',\n",
       "  'bitches',\n",
       "  'due',\n",
       "  'to',\n",
       "  'their',\n",
       "  'lack',\n",
       "  'of',\n",
       "  'street',\n",
       "  'skills',\n",
       "  'or',\n",
       "  'prison',\n",
       "  'experience',\n",
       "  'watching',\n",
       "  'oz',\n",
       "  'you',\n",
       "  'may',\n",
       "  'become',\n",
       "  'comfortable',\n",
       "  'with',\n",
       "  'what',\n",
       "  'is',\n",
       "  'uncomfortable',\n",
       "  'viewing',\n",
       "  'thats',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'get',\n",
       "  'in',\n",
       "  'touch',\n",
       "  'with',\n",
       "  'your',\n",
       "  'darker',\n",
       "  'side'],\n",
       " ['a',\n",
       "  'wonderful',\n",
       "  'little',\n",
       "  'production',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'filming',\n",
       "  'technique',\n",
       "  'is',\n",
       "  'very',\n",
       "  'unassuming',\n",
       "  'very',\n",
       "  'old',\n",
       "  'time',\n",
       "  'bbc',\n",
       "  'fashion',\n",
       "  'and',\n",
       "  'gives',\n",
       "  'a',\n",
       "  'comforting',\n",
       "  'and',\n",
       "  'sometimes',\n",
       "  'discomforting',\n",
       "  'sense',\n",
       "  'of',\n",
       "  'realism',\n",
       "  'to',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'piece',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'actors',\n",
       "  'are',\n",
       "  'extremely',\n",
       "  'well',\n",
       "  'chosen',\n",
       "  'michael',\n",
       "  'sheen',\n",
       "  'not',\n",
       "  'only',\n",
       "  'has',\n",
       "  'got',\n",
       "  'all',\n",
       "  'the',\n",
       "  'polari',\n",
       "  'but',\n",
       "  'he',\n",
       "  'has',\n",
       "  'all',\n",
       "  'the',\n",
       "  'voices',\n",
       "  'down',\n",
       "  'pat',\n",
       "  'too',\n",
       "  'you',\n",
       "  'can',\n",
       "  'truly',\n",
       "  'see',\n",
       "  'the',\n",
       "  'seamless',\n",
       "  'editing',\n",
       "  'guided',\n",
       "  'by',\n",
       "  'the',\n",
       "  'references',\n",
       "  'to',\n",
       "  \"williams'\",\n",
       "  'diary',\n",
       "  'entries',\n",
       "  'not',\n",
       "  'only',\n",
       "  'is',\n",
       "  'it',\n",
       "  'well',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'watching',\n",
       "  'but',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'terrificly',\n",
       "  'written',\n",
       "  'and',\n",
       "  'performed',\n",
       "  'piece',\n",
       "  'a',\n",
       "  'masterful',\n",
       "  'production',\n",
       "  'about',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'great',\n",
       "  \"master's\",\n",
       "  'of',\n",
       "  'comedy',\n",
       "  'and',\n",
       "  'his',\n",
       "  'life',\n",
       "  'br',\n",
       "  'br',\n",
       "  'the',\n",
       "  'realism',\n",
       "  'really',\n",
       "  'comes',\n",
       "  'home',\n",
       "  'with',\n",
       "  'the',\n",
       "  'little',\n",
       "  'things',\n",
       "  'the',\n",
       "  'fantasy',\n",
       "  'of',\n",
       "  'the',\n",
       "  'guard',\n",
       "  'which',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'use',\n",
       "  'the',\n",
       "  'traditional',\n",
       "  \"'dream'\",\n",
       "  'techniques',\n",
       "  'remains',\n",
       "  'solid',\n",
       "  'then',\n",
       "  'disappears',\n",
       "  'it',\n",
       "  'plays',\n",
       "  'on',\n",
       "  'our',\n",
       "  'knowledge',\n",
       "  'and',\n",
       "  'our',\n",
       "  'senses',\n",
       "  'particularly',\n",
       "  'with',\n",
       "  'the',\n",
       "  'scenes',\n",
       "  'concerning',\n",
       "  'orton',\n",
       "  'and',\n",
       "  'halliwell',\n",
       "  'and',\n",
       "  'the',\n",
       "  'sets',\n",
       "  'particularly',\n",
       "  'of',\n",
       "  'their',\n",
       "  'flat',\n",
       "  'with',\n",
       "  \"halliwell's\",\n",
       "  'murals',\n",
       "  'decorating',\n",
       "  'every',\n",
       "  'surface',\n",
       "  'are',\n",
       "  'terribly',\n",
       "  'well',\n",
       "  'done']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo\n",
    "sentence_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "### 2 - Crear los vectores (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "i0wnDdv9sJ47"
   },
   "outputs": [],
   "source": [
    "# Crearmos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=300,       # dimensionalidad de los vectores \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5lTt8wErsf17"
   },
   "outputs": [],
   "source": [
    "# Obtener el vocabulario con los tokens\n",
    "w2v_model.build_vocab(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "TNc9qt4os5AT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 50000\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "idw9cHF3tSMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus: 42635\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC9mZ8DPk-UC"
   },
   "source": [
    "### 3 - Entrenar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "QSp-x0PAsq56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 68993760.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo generador de vectores\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Utilizamos nuestro callback\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\badesia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model.train(sentence_tokens,\n",
    "                 total_examples=w2v_model.corpus_count,\n",
    "                 epochs=20,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddT9NVuNlCAe"
   },
   "source": [
    "### 4 - Ensayar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cHN9xGLuPEm"
   },
   "outputs": [],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"darling\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47HiU5gdkdMq"
   },
   "outputs": [],
   "source": [
    "# Palabras que MENOS se relacionan con...:\n",
    "w2v_model.wv.most_similar(negative=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT4Rvno2mD65"
   },
   "outputs": [],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"four\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPLDPgzBmQXt"
   },
   "outputs": [],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"money\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_UvHPMMklOr"
   },
   "outputs": [],
   "source": [
    "# Ensayar con una palabra que no está en el vocabulario:\n",
    "w2v_model.wv.most_similar(negative=[\"diedaa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el método `get_vector` permite obtener los vectores:\n",
    "vector_love = w2v_model.wv.get_vector(\"love\")\n",
    "print(vector_love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el método `most_similar` también permite comparar a partir de vectores\n",
    "w2v_model.wv.most_similar(vector_love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g8UVWe6lFmh"
   },
   "source": [
    "### 5 - Visualizar agrupación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDxEVXAivjr9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCCXtDpcugmd"
   },
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 3D\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model,3)\n",
    "\n",
    "fig = px.scatter_3d(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], z=vecs[:MAX_WORDS,2],text=labels[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También se pueden guardar los vectores y labels como tsv para graficar en\n",
    "# http://projector.tensorflow.org/\n",
    "\n",
    "\n",
    "vectors = np.asarray(w2v_model.wv.vectors)\n",
    "labels = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
    "\n",
    "with open(\"labels.tsv\", \"w\") as fp:\n",
    "    for item in labels:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
